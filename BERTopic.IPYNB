{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93cfa06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keybert import KeyBERT\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import unicodedata\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "# Load model directly\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, PartOfSpeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d054533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yue Qiao\\AppData\\Local\\Temp\\ipykernel_19400\\4014951015.py:3: DtypeWarning:\n",
      "\n",
      "Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the merged data\n",
    "input_path = r\"E:\\Waterloo-onedrive\\OneDrive - University of Waterloo\\MSE 641\\Merged Data\\Merged_data_ML.csv\"\n",
    "df_ml = pd.read_csv(input_path, sep=',', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a65c3b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the normalization function\n",
    "def normalize_text(text):\n",
    "    # 1. Unicode standardization\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    # 2. Lowercase conversion\n",
    "    text = text.lower()\n",
    "    # 3. Remove HTML tags and references\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    text = re.sub(r'\\[\\d+\\]', '', text)\n",
    "    # 4. Replace non-alphanumeric characters with space\n",
    "    text = re.sub(r'[^\\w\\s.!?]', ' ', text)\n",
    "    # 5. Merge multiple spaces into one\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Apply to the DataFrame\n",
    "df_ml['clean_text'] = df_ml['patent_text'].fillna(\"\").apply(normalize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bd34a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pub_dt', 'predict93_ml', 'ai_score_ml', 'predict93_evo', 'ai_score_evo', 'predict93_nlp', 'ai_score_nlp', 'predict93_speech', 'ai_score_speech', 'predict93_vision', 'ai_score_vision', 'predict93_planning', 'ai_score_planning', 'predict93_kr', 'ai_score_kr', 'predict93_hardware', 'ai_score_hardware', 'patent_id', 'patent_type', 'patent_title', 'num_claims', 'disambig_assignee_organization', 'assignee_type', 'patent_abstract', 'patent_text', 'clean_text']\n"
     ]
    }
   ],
   "source": [
    "print(df_ml.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83b52c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 High frequency words： [('image', np.float64(824.2106871660712)), ('model', np.float64(728.8277564517223)), ('network', np.float64(643.9653815249948)), ('plurality', np.float64(598.5892419474391)), ('learning', np.float64(558.1022608280825)), ('second', np.float64(540.318411762814)), ('using', np.float64(519.7525756753726)), ('input', np.float64(489.6430006511739)), ('machine', np.float64(461.70585446899304)), ('neural', np.float64(448.9299799688211)), ('content', np.float64(445.1731601385512)), ('training', np.float64(434.30426884814676)), ('systems', np.float64(427.07692674879223)), ('methods', np.float64(423.1146390849231)), ('computer', np.float64(415.3510781972855)), ('associated', np.float64(412.21876217530996)), ('machine learning', np.float64(394.8253492041094)), ('feature', np.float64(389.79874815057417)), ('neural network', np.float64(375.1116120876223)), ('object', np.float64(366.8138859259728))]\n"
     ]
    }
   ],
   "source": [
    "# High Frequency Words Extraction\n",
    "\n",
    "# 1. Self-contained stop words for domain-specific filtering\n",
    "domain_stop_words = {\n",
    "    'method', 'system', 'invention', 'apparatus',\n",
    "    'device', 'provide', 'include', 'comprise', 'includes',\n",
    "    'comprising', 'comprises', 'wherein', 'whereby',\n",
    "    'embodiment', 'disclosure', 'application', 'patent',\n",
    "    'user', 'information', 'processing',\n",
    "    'set', 'based','gain','data'\n",
    "}\n",
    "custom_stop_words = list(ENGLISH_STOP_WORDS.union(domain_stop_words))\n",
    "\n",
    "# 2. Load the dataset\n",
    "df_ml['pub_dt'] = pd.to_datetime(\n",
    "    df_ml['pub_dt'],\n",
    "    errors='raise',              \n",
    "    infer_datetime_format=True   \n",
    ")\n",
    "texts = df_ml['clean_text'].astype(str).tolist()\n",
    "timestamps = df_ml['pub_dt'].to_list()\n",
    "\n",
    "# 3. Identify the high-frequency words using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=custom_stop_words, max_df=0.9, min_df=1, ngram_range=(1, 5))\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(texts)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_scores = tfidf_matrix.sum(axis=0).A1\n",
    "word_scores = dict(zip(feature_names, tfidf_scores))\n",
    "\n",
    "# 4. Sort and display the top 20 high-frequency words\n",
    "sorted_words = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"Top 20 High frequency words：\", sorted_words[:20])\n",
    "\n",
    "# # Generate a word cloud for visualization, used for future reference\n",
    "# wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_scores)\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.imshow(wordcloud, interpolation='bilinear')\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46cb5b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Yue\n",
      "[nltk_data]     Qiao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "Splitting sentences: 100%|██████████| 87678/87678 [00:05<00:00, 16984.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of sentences: 388970\n",
      "Length of doc_ids: 388970\n",
      "Sentence count per document:\n",
      "count    87678.000000\n",
      "mean         4.929960\n",
      "std          2.091476\n",
      "min          1.000000\n",
      "25%          3.000000\n",
      "50%          5.000000\n",
      "75%          6.000000\n",
      "max         32.000000\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b1fc2c301aa4c9f91c8879b81190454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/6078 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Embedding time cost: 600 seconds\n",
      "▶ UMAP time: 282 seconds\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import psutil\n",
    "import numpy as np\n",
    "# Sentence analysis and embedding generation test\n",
    "# Download punkt_tab to fix LookupError\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# 1. Split sentences from the cleaned text and keep the index of the document\n",
    "sentences = []\n",
    "doc_ids = []\n",
    "for doc_id, text in enumerate(tqdm(df_ml['clean_text'].astype(str), desc=\"Splitting sentences\")):\n",
    "    doc_sentences = sent_tokenize(text)\n",
    "    for sentence in doc_sentences:\n",
    "        if len(sentence.split()) >= 8:\n",
    "            sentences.append(sentence)\n",
    "            doc_ids.append(doc_id)\n",
    "\n",
    "# Validate the length of sentences and doc_ids\n",
    "print(f\"Length of sentences: {len(sentences)}\")\n",
    "print(f\"Length of doc_ids: {len(doc_ids)}\")\n",
    "sentence_df = pd.DataFrame({'sentence': sentences, 'doc_id': doc_ids})\n",
    "end_preprocess = time.time()\n",
    "# print(f\"▶ Preprocessing time cost: {end_preprocess - start_preprocess:.0f} seconds\")\n",
    "# print(f\"▶ Number of sentences: {len(sentences)}\")\n",
    "# Validate the split sentences\n",
    "sentence_counts = [len(sent_tokenize(text)) for text in df_ml['clean_text'].astype(str)]\n",
    "print(\"Sentence count per document:\")\n",
    "print(pd.Series(sentence_counts).describe())\n",
    "\n",
    "# 2. Generate embeddings for the sentences using SentenceTransformer\n",
    "start_embed = time.time()\n",
    "embed_model = SentenceTransformer(\"AI-Growth-Lab/PatentSBERTa\", device=\"cuda\")\n",
    "sentence_embeddings = embed_model.encode(\n",
    "    sentences,\n",
    "    batch_size=64,  \n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True\n",
    ")\n",
    "end_embed = time.time()\n",
    "print(f\"▶ Embedding time cost: {end_embed - start_embed:.0f} seconds\")\n",
    "\n",
    "\n",
    "# 3. Configure UMAP, HDBSCAN models, amd representation models\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=10, \n",
    "    n_components=5, \n",
    "    min_dist=0.02,\n",
    "    metric='cosine', \n",
    "    low_memory=True,\n",
    "    random_state=42\n",
    "    )\n",
    "\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=20, \n",
    "    min_samples=5,\n",
    "    cluster_selection_method=\"leaf\",\n",
    "    cluster_selection_epsilon=0.02,\n",
    "    prediction_data=True)\n",
    "\n",
    "# Representation models for BERTopic\n",
    "# KeyBERT\n",
    "keybert_model = KeyBERTInspired()\n",
    "# Part-of-Speech    \n",
    "pos_model = PartOfSpeech(\"en_core_web_sm\")\n",
    "# MMR\n",
    "mmr_model = MaximalMarginalRelevance(diversity=0.3)\n",
    "# All representation models\n",
    "representation_model = {\n",
    "    \"KeyBERT\": keybert_model,\n",
    "    \"MMR\": mmr_model,\n",
    "    \"POS\": pos_model\n",
    "}\n",
    "# 4. Optional: Batch UMAP\n",
    "batch_size = 100000\n",
    "umap_embeddings = []\n",
    "start_umap = time.time()\n",
    "for i in range(0, len(sentence_embeddings), batch_size):\n",
    "    batch_embeddings = sentence_embeddings[i:i+batch_size]\n",
    "    batch_umap = umap_model.fit_transform(batch_embeddings)\n",
    "    umap_embeddings.append(batch_umap)\n",
    "    \n",
    "umap_embeddings = np.vstack(umap_embeddings)\n",
    "end_umap = time.time()\n",
    "print(f\"▶ UMAP time: {end_umap - start_umap:.0f} seconds\")\n",
    "\n",
    "# 5. Define the CountVectorizer used for c-TF-IDF\n",
    "vectorizer_model = CountVectorizer(\n",
    "    stop_words=custom_stop_words,\n",
    "    max_df=0.8,  # More lenient filtering to match c-TF-IDF settings\n",
    "    min_df=5,\n",
    "    ngram_range=(1, 3),\n",
    "    max_features=10000  # Set the maximum number of features to 10,000\n",
    ")\n",
    "# 6. Define seed topics for BERTopic\n",
    "seed_topic_list = [\n",
    "    [\"neural\", \"network\", \"deep\", \"learning\", \"convolutional\", \"recurrent\"],\n",
    "    [\"support\", \"vector\", \"machine\", \"svm\", \"kernel\"],\n",
    "    [\"decision\", \"tree\", \"random\", \"forest\", \"gradient\", \"boosting\"],\n",
    "    [\"reinforcement\", \"learning\", \"agent\", \"policy\", \"reward\"],\n",
    "    [\"natural\", \"language\", \"processing\", \"nlp\", \"transformer\", \"attention\", \"bert\"],\n",
    "    [\"clustering\", \"kmeans\", \"hierarchical\", \"dbscan\"],\n",
    "    [\"graph\", \"node\", \"edge\", \"gnn\", \"embedding\"],\n",
    "    [\"generative\", \"adversarial\", \"network\", \"gan\"],\n",
    "    [\"transfer\", \"learning\", \"domain\", \"adaptation\"],\n",
    "    [\"bayesian\", \"inference\", \"probabilistic\", \"model\"],\n",
    "    [\"computer\", \"vision\", \"image\", \"segmentation\"],\n",
    "    [\"unsupervised\", \"learning\", \"autoencoder\", \"vae\"],\n",
    "    [\"time\", \"series\", \"forecasting\", \"lstm\"],\n",
    "    [\"pca\", \"tsne\", \"umap\", \"reduction\"],\n",
    "    [\"anomaly\", \"outlier\", \"detection\", \"rare\"],\n",
    "    [\"gradient\", \"descent\", \"optimizer\", \"adam\", \"sgd\"],\n",
    "    [\"fairness\", \"bias\", \"explainable\", \"interpretability\", \"shap\", \"lime\"],\n",
    "    [\"federated\", \"privacy\", \"secure\", \"decentralized\"],\n",
    "    [\"automl\", \"nas\", \"search\", \"hyperparameter\", \"tuning\"],\n",
    "    [\"system\", \"module\", \"processor\", \"memory\", \"plurality\"],              \n",
    "    [\"data\", \"processing\", \"input\", \"output\", \"feature\", \"extraction\"],   \n",
    "    [\"model\", \"training\", \"learning\", \"fine-tuning\", \"optimization\"],     \n",
    "    [\"image\", \"segmentation\", \"detection\", \"reconstruction\", \"processing\"],\n",
    "    [\"automl\", \"nas\", \"search\", \"hyperparameter\", \"tuning\"],\n",
    "    [\"patent\", \"invention\", \"claim\", \"method\", \"apparatus\", \"system\"]\n",
    "]\n",
    "\n",
    "# 7. Initialize and fit the BERTopic model\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embed_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer_model,  # Use CountVectorizer\n",
    "    representation_model=representation_model,\n",
    "    # Hyperparameters\n",
    "    top_n_words=10,\n",
    "    min_topic_size=20,\n",
    "    # nr_topics=80,\n",
    "    seed_topic_list=seed_topic_list,\n",
    "    calculate_probabilities=False,\n",
    "    nr_topics=200\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee7e3475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embedding is saved to: E:\\Waterloo-onedrive\\OneDrive - University of Waterloo\\MSE 641\\Embeddings\\embeddings.npy\n"
     ]
    }
   ],
   "source": [
    "# Save the sentence embeddings to a file\n",
    "import numpy as np, os\n",
    "save_dir = r\"E:\\Waterloo-onedrive\\OneDrive - University of Waterloo\\MSE 641\\Embeddings\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "emb_path = os.path.join(save_dir, \"embeddings.npy\")\n",
    "np.save(emb_path, sentence_embeddings)\n",
    "\n",
    "print(\"✅ Embedding is saved to:\", emb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b72e0b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ BERTopic Cludester and topic extraction cost：841 seconds\n"
     ]
    }
   ],
   "source": [
    "# When to load the embeddings later, uncomment the following line:\n",
    "#embeddings = np.load(r\"E:\\Waterloo-onedrive\\OneDrive - University of Waterloo\\MSE 641\\Embeddings\\embeddings.npy\")\n",
    "# Start the topic modeling process\n",
    "start_tm = time.time()\n",
    "topics = topic_model.fit_transform(\n",
    "    sentences,\n",
    "    sentence_embeddings  \n",
    ")\n",
    "end_tm = time.time()\n",
    "print(f\"▶ BERTopic Cludester and topic extraction cost：{end_tm - start_tm:.0f} seconds\")\n",
    "\n",
    "# # Time prediction (training phase) \n",
    "# fit_time_per_sentence = (end_tm - start_tm) / len(sentences)\n",
    "# estimated_fit_time = fit_time_per_sentence * full_sentence_estimate\n",
    "# print(f\"▶ Estimated training time for full dataset ({full_sentence_estimate:.0f} sentences): {estimated_fit_time:.0f} seconds\")\n",
    "# print(f\"▶ Estimated total time (embedding + training): {estimated_embed_time + estimated_fit_time:.0f} seconds\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac434289",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-01 22:39:59,819 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model is saved to： E:\\Waterloo-onedrive\\OneDrive - University of Waterloo\\MSE 641\\BERTopic model\\my_bertopic_model\n",
      "✅ topics.pkl Save completed： E:\\Waterloo-onedrive\\OneDrive - University of Waterloo\\MSE 641\\BERTopic model\\topics.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save the model and topics\n",
    "\n",
    "import pickle\n",
    "save_dir = r\"E:\\Waterloo-onedrive\\OneDrive - University of Waterloo\\MSE 641\\BERTopic model\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "model_path  = os.path.join(save_dir, \"my_bertopic_model\")  \n",
    "topics_path = os.path.join(save_dir, \"topics.pkl\")\n",
    "#probs_path  = os.path.join(save_dir, \"probs.npy\")\n",
    "\n",
    "topic_model.save(model_path, serialization=\"pickle\")\n",
    "# with open(topics_path, \"wb\") as f:\n",
    "#     pickle.dump(topics, f)\n",
    "# np.save(probs_path)\n",
    "\n",
    "print(\"✅ Model is saved to：\", model_path)\n",
    "print(\"✅ topics.pkl Save completed：\", topics_path)\n",
    "#print(\"✅ probs.npy  Save completed\", probs_path, f\"(shape: {probs.shape})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49d82091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and transform the sentences\n",
    "topics, probs = topic_model.transform(\n",
    "  sentences,\n",
    "  embeddings=sentence_embeddings\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9892bf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce outliers (Topic -1) using c-TF-IDF\n",
    "\n",
    "new_topics = topic_model.reduce_outliers(   \n",
    "    sentences,\n",
    "    topics,\n",
    "    strategy=\"c-tf-idf\",\n",
    "    threshold=0.05  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb73b932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update topics with the new topics\n",
    "topic_model.update_topics(\n",
    "    sentences,      # docs 列表\n",
    "    new_topics,     # topics 列表\n",
    "    vectorizer_model=vectorizer_model\n",
    ")\n",
    "\n",
    "topics = new_topics\n",
    "# from collections import Counter\n",
    "\n",
    "# print(\"new_topics distributions:\", Counter(new_topics))              \n",
    "# print(topic_model.get_topic_info().head())  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1234af05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ topics.pkl is saved to：E:\\Waterloo-onedrive\\OneDrive - University of Waterloo\\MSE 641\\BERTopic model\\topics.pkl\n",
      "✅ probs.npy is saved to: E:\\Waterloo-onedrive\\OneDrive - University of Waterloo\\MSE 641\\BERTopic model\\probs.npy (shape: (388970,))\n",
      "✅ doc_ids.pkl is saved to:E:\\Waterloo-onedrive\\OneDrive - University of Waterloo\\MSE 641\\BERTopic model\\doc_ids.pkl\n"
     ]
    }
   ],
   "source": [
    "# save the new topics and probs\n",
    "save_dir = r\"E:\\Waterloo-onedrive\\OneDrive - University of Waterloo\\MSE 641\\BERTopic model\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "topics_path = os.path.join(save_dir, \"topics.pkl\")\n",
    "probs_path  = os.path.join(save_dir, \"probs.npy\")\n",
    "with open(topics_path, \"wb\") as f:\n",
    "    pickle.dump(topics, f)\n",
    "\n",
    "# 5. Save the probabilities\n",
    "np.save(probs_path, probs)\n",
    "\n",
    "print(f\"✅ topics.pkl is saved to：{topics_path}\")\n",
    "print(f\"✅ probs.npy is saved to: {probs_path} (shape: {probs.shape})\")\n",
    "\n",
    "doc_ids_path = os.path.join(save_dir, \"doc_ids.pkl\")\n",
    "with open(doc_ids_path, \"wb\") as f:\n",
    "    pickle.dump(doc_ids, f)\n",
    "print(f\"✅ doc_ids.pkl is saved to:{doc_ids_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26560349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #For loading the model and topics later, you can use the following code:\n",
    "from bertopic import BERTopic\n",
    "import pickle, numpy as np\n",
    "import pandas as pd\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c812ed3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir   = r\"E:\\Waterloo-onedrive\\OneDrive - University of Waterloo\\MSE 641\\BERTopic model\\my_bertopic_model\"\n",
    "topics_path = r\"E:\\Waterloo-onedrive\\OneDrive - University of Waterloo\\MSE 641\\BERTopic model\\topics.pkl\"\n",
    "probs_path  = r\"E:\\Waterloo-onedrive\\OneDrive - University of Waterloo\\MSE 641\\BERTopic model\\probs.npy\"\n",
    "docs_path   = r\"E:\\Waterloo-onedrive\\OneDrive - University of Waterloo\\MSE 641\\BERTopic model\\doc_ids.pkl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9db3d8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove repulication and the result is saved：\n",
      "   - topic_info.csv\n",
      "   - topic_id_name.csv ）\n",
      "   - docs_topics.csv \n"
     ]
    }
   ],
   "source": [
    "\n",
    "topic_model = BERTopic.load(model_dir)\n",
    "with open(topics_path, \"rb\") as f:\n",
    "    topics = pickle.load(f)  # list of topic_ids\n",
    "\n",
    "\n",
    "probs = np.load(probs_path, allow_pickle=True)  # shape: (n_docs, n_topics)\n",
    "\n",
    "with open(docs_path, \"rb\") as f:\n",
    "    doc_ids = pickle.load(f)\n",
    "\n",
    "\n",
    "# 1. Extract and export topic information (improve deduplication)\n",
    "out_dir = os.path.dirname(model_dir)\n",
    "topic_info = topic_model.get_topic_info()\n",
    "\n",
    "\n",
    "def dedup_phrases(phrase_list):\n",
    "    cleaned = []\n",
    "    for p in phrase_list:\n",
    "        if any(p in q.split() for q in cleaned if p != q):\n",
    "            continue\n",
    "        cleaned = [q for q in cleaned if not (q in p.split() and q != p)]\n",
    "        cleaned.append(p)\n",
    "    return cleaned\n",
    "\n",
    "# 2. Transform the Name column to a list of phrases and deduplicate\n",
    "def clean_name(name_str):\n",
    "    phrases = name_str.split('_')        # The original phrases are separated by underscores\n",
    "    deduped = dedup_phrases(phrases)\n",
    "\n",
    "    return \"_\".join(deduped[:5])\n",
    "# Output the cleaned names\n",
    "topic_info['clean_name'] = topic_info['Name'].apply(clean_name)\n",
    "\n",
    "# Save the topic_info\n",
    "topic_info.to_csv(os.path.join(out_dir, \"topic_info.csv\"),\n",
    "                  index=False, encoding=\"utf-8-sig\")\n",
    "# Save ID→Name mapping\n",
    "topic_info[['Topic','clean_name']].to_csv(\n",
    "    os.path.join(out_dir, \"topic_id_name.csv\"),\n",
    "    index=False, encoding=\"utf-8-sig\"\n",
    ")\n",
    "\n",
    "# 3. Create a DataFrame for documents and their topics\n",
    "df_docs = pd.DataFrame({\n",
    "    \"doc_id\": doc_ids,\n",
    "    \"topic_id\": topics,\n",
    "    \"max_prob\": [p.max() for p in probs]\n",
    "})\n",
    "df_docs = df_docs.merge(\n",
    "    topic_info[['Topic','clean_name']],\n",
    "    left_on='topic_id', right_on='Topic',\n",
    "    how='left'\n",
    ").drop(columns=['Topic']).rename(columns={'clean_name':'topic_name'})\n",
    "df_docs.to_csv(os.path.join(out_dir, \"docs_topics.csv\"),\n",
    "               index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"Remove repulication and the result is saved：\")\n",
    "print(\"   - topic_info.csv\")\n",
    "print(\"   - topic_id_name.csv ）\")\n",
    "print(\"   - docs_topics.csv \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
